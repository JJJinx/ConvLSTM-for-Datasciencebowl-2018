{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable,Function\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import sys \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "plt.ion()   # interactive mode\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../DATA/stage1_train/'\n",
    "TEST_PATH = '../DATA/stage1_test/'\n",
    "UNET_PATH = '../DATA/unetdata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_f(nn.Module):\n",
    "    def __init__(self,lamda=1):\n",
    "        # super parameter\n",
    "        super(loss_f,self).__init__()\n",
    "        self.lamda = lamda\n",
    "    def forward(self,output_list,scores,gt_masks):\n",
    "        \"\"\"\n",
    "        gt_masks: sample['masks'],shape=[1,masks_number,H,W]\n",
    "        output_list: list length=[premasks_number],element shape = [H,W]\n",
    "        scores: list length = [premasks_numbers]\n",
    "        batchsize = 1\n",
    "        \"\"\"\n",
    "        gt_masks = gt_masks.squeeze(0) # max_element = 1\n",
    "        num_gt = gt_masks.size()[0]\n",
    "        num_pre = len(output_list) \n",
    "        num_min = min(num_gt,num_pre)\n",
    "        real_output = torch.stack(output_list) # shape = [premasks,H,W]\n",
    "        rps = torch.stack(scores)\n",
    "        loss = 0\n",
    "        Matrix = []\n",
    "        for i in range(num_min): # when pre>gt, drop the later prediction\n",
    "            for j in range(num_gt): \n",
    "                #generate a num_gt*num_pre matrix \n",
    "                IoU = self._iou(real_output[i],gt_masks[j]) ## IoU is a tensor scalar\n",
    "                Matrix.append(IoU)\n",
    "\n",
    "        Matrix = torch.stack(Matrix).view(num_min,num_gt)\n",
    "        M = hungarian()(Matrix)\n",
    "#         print('-M',torch.sum(-M).item())\n",
    "#         print('rps=0',rps[0])\n",
    "#         print('rps=num_min',rps[num_min-1])\n",
    "        for i in range(num_min):\n",
    "            loss = loss + (M[i]+self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([1.0]).to(device)))\n",
    "        for i in range(num_gt,num_pre):\n",
    "            loss = loss + self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([0.0]).to(device)) #F.binary_cross_entropy(b,a)\n",
    "\n",
    "        return loss\n",
    "    def _iou(self,x,y):\n",
    "        iou_inter = torch.sum(torch.mul(x,y))\n",
    "        iou = iou_inter/(torch.sum(x)+torch.sum(y)-iou_inter)\n",
    "        return -iou\n",
    "    \n",
    "class hungarian(Function):\n",
    "\n",
    "    def forward(ctx,input):\n",
    "        numpy_input = input.cpu().detach().numpy()\n",
    "        matrix = np.zeros_like(numpy_input)\n",
    "        row_ind,col_ind = linear_sum_assignment(numpy_input)\n",
    "        length = len(row_ind)\n",
    "        M = []\n",
    "        for i in range(length):\n",
    "            matrix[row_ind[i]][col_ind[i]] = numpy_input[row_ind[i]][col_ind[i]]\n",
    "            M.append(numpy_input[row_ind[i]][col_ind[i]])\n",
    "        M = np.stack(M)\n",
    "        matrix = torch.from_numpy(matrix).cuda()\n",
    "        ctx.save_for_backward(matrix)\n",
    "        result = torch.from_numpy(M).cuda()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def backward(ctx,grad_output):\n",
    "        \n",
    "        matrix,=ctx.saved_tensors\n",
    "        grad_input = matrix.clone()\n",
    "        return grad_input\n",
    "\n",
    "#         result = torch.from_numpy(matrix).type(torch.FloatTensor).cuda()\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x64 = self.up4(x, x1)\n",
    "        x = self.outc(x64)\n",
    "        return (F.sigmoid(x),x64)\n",
    "\n",
    "# sub-parts of the U-Net model\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()       \n",
    "        # ConvLSTM part\n",
    "        self.conv = nn.Conv2d(64,16,3,stride=1,padding=1)\n",
    "        self.convlstm = ConvLSTM(input_size=(176,176),\n",
    "                                 input_dim=16,\n",
    "                                 hidden_dim=[16],\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 num_layers=1,\n",
    "                                 batch_first=False,\n",
    "                                 bias=True,\n",
    "                                 return_all_layers=True)\n",
    "        # SI part\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(16,1,1,stride=1,padding=0)\n",
    "        self.fc = nn.Linear(16*176*176, 1)\n",
    "        \n",
    "    def forward(self,x,score_bound,seq_len):\n",
    "        # x.shape() = [1,64,176,176]\n",
    "        x = self.conv(x)#[1,16,176,176]\n",
    "        size = x.size()[-1] # W\n",
    "        # convlstm and si\n",
    "        convlstm_input = x.unsqueeze(0) #[1,batch,d,h,w],(t, b, c, h, w) -> (b, t, c, h, w) (i.e.[1,1,16,176,176])\n",
    "        output_list = []\n",
    "        scores = []\n",
    "        hidden_state=None\n",
    "        for i in range(seq_len):\n",
    "            [layer_output_list, last_state_list] = self.convlstm(convlstm_input,hidden_state) # layer_output_list=[1,batch,t,d,h,w],t=1,(i.e.[1,1,16,176,176])\n",
    "                                                                                 # last_state_list(i.e. [[h,c]])=[1,2,batch,t,d,h,w] (i.e.[1,2,1,1,16,176,176])\n",
    "            hidden_state = last_state_list\n",
    "\n",
    "            layer_output_list = layer_output_list[0].squeeze()\n",
    "            \n",
    "            # produce score\n",
    "            score_input = layer_output_list.unsqueeze(0)\n",
    "            #score_input = F.max_pool2d(score_input,2) #[88,88]\n",
    "            [b,c,h,w] = score_input.size()\n",
    "            score = F.sigmoid(self.fc(score_input.view(b*c*h*w)))\n",
    "            scores.append(score)\n",
    "            # produce masks\n",
    "            SI = F.sigmoid(\n",
    "                    F.log_softmax(\n",
    "                        self.conv8(layer_output_list.unsqueeze(0)))) #the input is [1,d,h,w],SI is [h,w] (i.e.[16,176,176])\n",
    "\n",
    "            mask = SI.squeeze() #[batch,h,w] ,since batchsize=1,it should be [h,w] (i.e.[176,176])\n",
    "            output_list.append(mask)\n",
    "        return (output_list,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDataset(Dataset):\n",
    "    \"\"\"Neural dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the examples.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.example_list = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.example_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example_dir = os.path.join(self.root_dir,\n",
    "                                self.example_list[idx])\n",
    "        img_dir = example_dir+'/images'\n",
    "        masks_dir = example_dir+'/masks'\n",
    "        \n",
    "        img_name = img_dir+'/'+os.listdir(img_dir)[0]\n",
    "        image = io.imread(img_name)[:,:,0:3]\n",
    "        \n",
    "        maskwalk = os.walk(masks_dir).__next__()\n",
    "        masks = []\n",
    "        for item in maskwalk[2]:\n",
    "            masks_name = os.path.join(masks_dir,item)\n",
    "            mask = io.imread(masks_name)\n",
    "            masks.append(mask)\n",
    "        masks = np.stack(masks)\n",
    "        \n",
    "        sample = {'image': image, 'masks': masks}# masks is [masknumber,H,W],image [H,W,C]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # resize the masks\n",
    "        new_msk = []\n",
    "        for mask in masks:\n",
    "            new_msk.append(transform.resize(mask, (new_h, new_w)))\n",
    "        new_mask = np.stack(new_msk)\n",
    "        \n",
    "        return {'image': img, 'masks': new_mask}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        if h-new_h==0:\n",
    "            top = 0\n",
    "        else:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        if w-new_w==0:\n",
    "            left = 0\n",
    "        else:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        # crop the masks\n",
    "        new_msk = []\n",
    "        for mask in masks:\n",
    "            newm = mask[top: top + new_h,\n",
    "                        left: left + new_w]\n",
    "            new_msk.append(newm)\n",
    "        new_mask = np.stack(new_msk)\n",
    "\n",
    "        return {'image': image, 'masks': new_mask}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        # numpy masks: N x H x W\n",
    "        # torch masks: N X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'masks': torch.from_numpy(masks)}\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        # input_tensor is [batch,channel,h,w]\n",
    "        h_cur, c_cur = cur_state     \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        \n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next) # size of h and c is [batch,channel,h,w]\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda(),\n",
    "                Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda()]\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo \n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list   = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):  #num_layers should be 2\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])# size of h and c is [batch,c,h,w]\n",
    "                output_inner.append(h)#[t,batch,c,h,w]\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)# layer_output is [batch,t,c,h,w]\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)# [1,batch,t,c,h,w]\n",
    "            last_state_list.append([h, c])#[1,2,batch,t,c,h,w]\n",
    "        torch.cuda.empty_cache()\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list   = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states # [[h,c]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "transformed_dataset = NeuralDataset(root_dir=TRAIN_PATH,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Rescale(176),\n",
    "                                        RandomCrop(176),\n",
    "                                        ToTensor()]))\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model\n",
    "unet = torch.load('unet.pkl')\n",
    "net = Net().to(device)\n",
    "# training\n",
    "torch.cuda.empty_cache()\n",
    "# create your optimizer\n",
    "criterion = loss_f()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01,)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 num_data= 0 seq_len= 2 loss 1.3240092992782593 lr 0.01\n",
      "epoch= 0 num_data= 1 seq_len= 2 loss 1.3303252458572388 lr 0.01\n",
      "epoch= 0 num_data= 2 seq_len= 2 loss 1.3340572118759155 lr 0.01\n",
      "epoch= 0 num_data= 3 seq_len= 2 loss 1.3330788612365723 lr 0.01\n",
      "epoch= 0 num_data= 4 seq_len= 2 loss 1.2698934078216553 lr 0.01\n",
      "epoch= 0 num_data= 5 seq_len= 2 loss 1.3371150493621826 lr 0.01\n",
      "epoch= 0 num_data= 6 seq_len= 2 loss 1.3181287050247192 lr 0.01\n",
      "epoch= 0 num_data= 7 seq_len= 2 loss 1.3149006366729736 lr 0.01\n",
      "epoch= 0 num_data= 8 seq_len= 2 loss 1.3416309356689453 lr 0.01\n",
      "epoch= 0 num_data= 9 seq_len= 2 loss 1.3403486013412476 lr 0.01\n",
      "epoch= 0 num_data= 10 seq_len= 2 loss 1.2429008483886719 lr 0.01\n",
      "epoch= 0 num_data= 11 seq_len= 2 loss 1.3313279151916504 lr 0.01\n",
      "epoch= 0 num_data= 12 seq_len= 2 loss 1.2857414484024048 lr 0.01\n",
      "epoch= 0 num_data= 13 seq_len= 2 loss 1.307157278060913 lr 0.01\n",
      "epoch= 0 num_data= 14 seq_len= 2 loss 1.2794618606567383 lr 0.01\n",
      "epoch= 0 num_data= 15 seq_len= 2 loss 1.291953682899475 lr 0.01\n",
      "epoch= 0 num_data= 16 seq_len= 2 loss 1.245729923248291 lr 0.01\n",
      "epoch= 0 num_data= 17 seq_len= 2 loss 1.3119803667068481 lr 0.01\n",
      "epoch= 0 num_data= 18 seq_len= 2 loss 1.3265608549118042 lr 0.01\n",
      "epoch= 0 num_data= 19 seq_len= 2 loss 1.2935335636138916 lr 0.01\n",
      "epoch= 0 num_data= 20 seq_len= 2 loss 1.3037831783294678 lr 0.01\n",
      "epoch= 0 num_data= 21 seq_len= 2 loss 1.2991832494735718 lr 0.001\n",
      "epoch= 0 num_data= 22 seq_len= 2 loss 1.2821919918060303 lr 0.001\n",
      "epoch= 0 num_data= 23 seq_len= 2 loss 1.286453127861023 lr 0.001\n",
      "epoch= 0 num_data= 24 seq_len= 2 loss 1.2958143949508667 lr 0.001\n",
      "epoch= 0 num_data= 25 seq_len= 2 loss 1.2975108623504639 lr 0.001\n",
      "epoch= 0 num_data= 26 seq_len= 2 loss 1.2561578750610352 lr 0.001\n",
      "epoch= 0 num_data= 27 seq_len= 2 loss 1.3084819316864014 lr 0.001\n",
      "epoch= 0 num_data= 28 seq_len= 2 loss 1.3044447898864746 lr 0.001\n",
      "epoch= 0 num_data= 29 seq_len= 2 loss 1.3031220436096191 lr 0.001\n",
      "epoch= 0 num_data= 30 seq_len= 2 loss 1.3161308765411377 lr 0.001\n",
      "epoch= 0 num_data= 31 seq_len= 2 loss 1.264904260635376 lr 0.001\n",
      "epoch= 0 num_data= 32 seq_len= 2 loss 1.30314040184021 lr 0.0001\n",
      "epoch= 0 num_data= 33 seq_len= 2 loss 1.2899504899978638 lr 0.0001\n",
      "epoch= 0 num_data= 34 seq_len= 2 loss 1.2870206832885742 lr 0.0001\n",
      "epoch= 0 num_data= 35 seq_len= 2 loss 1.2456974983215332 lr 0.0001\n",
      "epoch= 0 num_data= 36 seq_len= 2 loss 1.3079988956451416 lr 0.0001\n",
      "epoch= 0 num_data= 37 seq_len= 2 loss 1.3206557035446167 lr 0.0001\n",
      "epoch= 0 num_data= 38 seq_len= 2 loss 1.3103594779968262 lr 0.0001\n",
      "epoch= 0 num_data= 39 seq_len= 2 loss 1.4126613140106201 lr 0.0001\n",
      "epoch= 0 num_data= 40 seq_len= 2 loss 1.2363317012786865 lr 0.0001\n",
      "epoch= 0 num_data= 41 seq_len= 2 loss 1.2580777406692505 lr 0.0001\n",
      "epoch= 0 num_data= 42 seq_len= 2 loss 1.2982008457183838 lr 0.0001\n",
      "epoch= 0 num_data= 43 seq_len= 2 loss 1.3128453493118286 lr 0.0001\n",
      "epoch= 0 num_data= 44 seq_len= 2 loss 1.3080745935440063 lr 0.0001\n",
      "epoch= 0 num_data= 45 seq_len= 2 loss 1.2993264198303223 lr 0.0001\n",
      "epoch= 0 num_data= 46 seq_len= 2 loss 1.323737382888794 lr 0.0001\n",
      "epoch= 0 num_data= 47 seq_len= 2 loss 1.2944951057434082 lr 0.0001\n",
      "epoch= 0 num_data= 48 seq_len= 2 loss 1.3008837699890137 lr 0.0001\n",
      "epoch= 0 num_data= 49 seq_len= 2 loss 1.2408026456832886 lr 0.0001\n",
      "epoch= 0 num_data= 50 seq_len= 2 loss 1.2425267696380615 lr 0.0001\n",
      "epoch= 0 num_data= 51 seq_len= 2 loss 1.2783714532852173 lr 1e-05\n",
      "epoch= 0 num_data= 52 seq_len= 2 loss 1.3012759685516357 lr 1e-05\n",
      "epoch= 0 num_data= 53 seq_len= 2 loss 1.3161215782165527 lr 1e-05\n",
      "epoch= 0 num_data= 54 seq_len= 2 loss 1.3140950202941895 lr 1e-05\n",
      "epoch= 0 num_data= 55 seq_len= 2 loss 1.2791725397109985 lr 1e-05\n",
      "epoch= 0 num_data= 56 seq_len= 2 loss 1.3186650276184082 lr 1e-05\n",
      "epoch= 0 num_data= 57 seq_len= 2 loss 1.3211596012115479 lr 1e-05\n",
      "epoch= 0 num_data= 58 seq_len= 2 loss 1.2544516324996948 lr 1e-05\n",
      "epoch= 0 num_data= 59 seq_len= 2 loss 1.308161973953247 lr 1e-05\n",
      "epoch= 0 num_data= 60 seq_len= 2 loss 1.3293603658676147 lr 1e-05\n",
      "epoch= 0 num_data= 61 seq_len= 2 loss 1.310538649559021 lr 1e-05\n",
      "epoch= 0 num_data= 62 seq_len= 2 loss 1.294316291809082 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 63 seq_len= 2 loss 1.2952325344085693 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 64 seq_len= 2 loss 1.3179570436477661 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 65 seq_len= 2 loss 1.2892968654632568 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 66 seq_len= 2 loss 1.2953979969024658 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 67 seq_len= 2 loss 1.3016397953033447 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 68 seq_len= 2 loss 1.252772331237793 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 69 seq_len= 2 loss 1.2527759075164795 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 70 seq_len= 2 loss 1.3085561990737915 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 71 seq_len= 2 loss 1.2640697956085205 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 72 seq_len= 2 loss 1.3055063486099243 lr 1.0000000000000002e-06\n",
      "epoch= 0 num_data= 73 seq_len= 2 loss 1.279252290725708 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 74 seq_len= 2 loss 1.3158845901489258 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 75 seq_len= 2 loss 1.3100666999816895 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 76 seq_len= 2 loss 1.263350486755371 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 77 seq_len= 2 loss 1.2979021072387695 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 78 seq_len= 2 loss 1.3131887912750244 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 79 seq_len= 2 loss 1.2934889793395996 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 80 seq_len= 2 loss 1.2953720092773438 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 81 seq_len= 2 loss 1.3152837753295898 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 82 seq_len= 2 loss 1.3258850574493408 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 83 seq_len= 2 loss 1.3300776481628418 lr 1.0000000000000002e-07\n",
      "epoch= 0 num_data= 84 seq_len= 2 loss 1.3026597499847412 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 85 seq_len= 2 loss 1.319192886352539 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 86 seq_len= 2 loss 1.2169047594070435 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 87 seq_len= 2 loss 1.2813928127288818 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 88 seq_len= 2 loss 1.2956594228744507 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 89 seq_len= 2 loss 1.3006591796875 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 90 seq_len= 2 loss 1.299821138381958 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 91 seq_len= 2 loss 1.3012807369232178 lr 1.0000000000000004e-08\n",
      "epoch= 0 num_data= 92 seq_len= 2 loss 1.2650697231292725 lr 1.0000000000000004e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-daa25633b0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# loop over the dataset multiple times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[1;31m# get the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a6ae4d3b2f08>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a6ae4d3b2f08>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mnew_msk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mnew_msk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mnew_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_msk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\skimage\\transform\\_warps.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mtform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffineTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[0mtform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_corners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_corners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         out = warp(image, tform, output_shape=output_shape, order=order,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\skimage\\transform\\_geometric.py\u001b[0m in \u001b[0;36mestimate\u001b[1;34m(self, src, dst)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coeffs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs\n",
    "        inputs = (data['image'].type(torch.float32)).to(device)\n",
    "        labels = data['masks'].type(torch.float32).to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        seq_len = 2\n",
    "        with torch.no_grad():\n",
    "            (x,x64) = unet(inputs)\n",
    "        (output_list,scores) = net(x64,0.5,seq_len)\n",
    "        loss = criterion(output_list,scores,labels) #  the minimum of loss is -1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(filter(lambda p:p.requires_grad,net.parameters()), max_norm=5, norm_type=1)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        \n",
    "        #print('epoch=',epoch,'num_data=',i,'seq_len=',seq_len,'loss',loss.item(),'lr',optimizer.param_groups[0]['lr'])\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 40 == 39:    # print every 134 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f,seq_len:%d' %\n",
    "                  (epoch + 1, i + 1, running_loss /134,seq_len))\n",
    "            running_loss = 0.0\n",
    "#             torch.save(net.state_dict(), '../mini_program/net_params.pkl')\n",
    "        \n",
    "    if seq_len>=420:\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgpu",
   "language": "python",
   "name": "pytorchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
