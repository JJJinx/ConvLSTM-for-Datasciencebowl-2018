{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable,Function\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import sys \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "plt.ion()   # interactive mode\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'convlstm',\n",
       " 'DATA',\n",
       " 'literature review',\n",
       " 'markingbands.pdf',\n",
       " 'maskrcnn',\n",
       " 'My First Project-425e70c20cb8.json',\n",
       " 'ppt',\n",
       " 'pytorch_convlstm',\n",
       " 'RIS-master',\n",
       " 'term2miniproject-markingsheet-1.pdf',\n",
       " 'Unet',\n",
       " 'Writing Mini-project and Project Reports.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../DATA/stage1_train/'\n",
    "TEST_PATH = '../DATA/stage1_test/'\n",
    "UNET_PATH = '../DATA/unetdata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_pre(nn.Module):\n",
    "    def __init__(self,lamda=1):\n",
    "        # super parameter\n",
    "        super(loss_pre,self).__init__()\n",
    "    def forward(self,fcn,mask):\n",
    "        fcn = fcn.squeeze()\n",
    "        loss = F.binary_cross_entropy(fcn,mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class loss_f(nn.Module):\n",
    "    def __init__(self,lamda=1):\n",
    "        # super parameter\n",
    "        super(loss_f,self).__init__()\n",
    "        self.lamda = lamda\n",
    "    def forward(self,output_list,scores,gt_masks):\n",
    "        \"\"\"\n",
    "        gt_masks: sample['masks'],shape=[1,masks_number,H,W]\n",
    "        output_list: list length=[premasks_number],element shape = [H,W]\n",
    "        scores: list length = [premasks_numbers]\n",
    "        batchsize = 1\n",
    "        \"\"\"\n",
    "        gt_masks = gt_masks.squeeze(0)\n",
    "        num_gt = gt_masks.size()[0]\n",
    "        num_pre = len(output_list) \n",
    "        real_output = torch.stack(output_list) # shape = [premasks,H,W]\n",
    "        rps = torch.stack(scores)\n",
    "        loss = 0\n",
    "        Matrix = []\n",
    "        for i in range(num_pre):\n",
    "            for j in range(num_gt): \n",
    "                #generate a num_gt*num_pre matrix \n",
    "                IoU = self._iou(real_output[i],gt_masks[j]) ## IoU is a tensor scalar\n",
    "                Matrix.append(IoU)\n",
    "\n",
    "        Matrix = torch.stack(Matrix).view(num_pre,num_gt)\n",
    "#         row_ind,col_ind = linear_sum_assignment(Matrix.detach().cpu().numpy())\n",
    "\n",
    "#         #print('num_gt=',num_gt,'num_pre=',num_pre,'score',rps[0:num_gt],rps[num_gt+20],rps[num_gt+40],rps[num_gt+60])\n",
    "#         print('rps 0',rps[0],F.binary_cross_entropy(rps[0],torch.tensor([0.0]).to(device)),'rps numgt+10',rps[num_gt+10],F.binary_cross_entropy(rps[num_gt+10],torch.tensor([0.0]).to(device)))\n",
    "#         for i in range(num_gt):\n",
    "#             loss = loss + (-Matrix[[row_ind[i]],[col_ind[i]]].to(device)+self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([1.0]).to(device)))\n",
    "#         for i in range(num_gt,num_pre):\n",
    "#             loss = loss + self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([0.0]).to(device)) #F.binary_cross_entropy(b,a)  \n",
    "        \n",
    "        M = hungarian()(Matrix)\n",
    "        for i in range(num_gt):\n",
    "            print('rps',i,rps[i].item(),F.binary_cross_entropy(rps[i],torch.tensor([1.0]).to(device)).item())\n",
    "            loss = loss + (-M[i]+self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([1.0]).to(device)))\n",
    "        for i in range(num_gt,num_pre):\n",
    "            print('rps',i,rps[i].item(),F.binary_cross_entropy(rps[i],torch.tensor([0.0]).to(device)).item())\n",
    "            loss = loss + self.lamda*F.binary_cross_entropy(rps[i],torch.tensor([0.0]).to(device)) #F.binary_cross_entropy(b,a)  \n",
    "        return loss\n",
    "    def _iou(self,x,y):\n",
    "        iou_inter = torch.sum(torch.mul(x,y))\n",
    "        iou = iou_inter/(torch.sum(x)+torch.sum(y)-iou_inter)\n",
    "        return iou\n",
    "    \n",
    "class hungarian(Function):\n",
    "\n",
    "    def forward(ctx,input):\n",
    "        numpy_input = input.cpu().detach().numpy()\n",
    "        matrix = np.zeros_like(numpy_input)\n",
    "        row_ind,col_ind = linear_sum_assignment(numpy_input)\n",
    "        length = len(row_ind)\n",
    "        M = []\n",
    "        for i in range(length):\n",
    "            matrix[row_ind[i]][col_ind[i]] = numpy_input[row_ind[i]][col_ind[i]]\n",
    "            M.append(numpy_input[row_ind[i]][col_ind[i]])\n",
    "        M = np.stack(M)\n",
    "        matrix = torch.from_numpy(matrix).cuda()\n",
    "        ctx.save_for_backward(matrix)\n",
    "        result = torch.from_numpy(M).cuda()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def backward(ctx,grad_output):\n",
    "        \n",
    "        matrix,=ctx.saved_tensors\n",
    "        grad_input = matrix.clone()\n",
    "        return grad_input\n",
    "\n",
    "#         result = torch.from_numpy(matrix).type(torch.FloatTensor).cuda()\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x64 = self.up4(x, x1)\n",
    "        x = self.outc(x64)\n",
    "        return (F.sigmoid(x),x64)\n",
    "\n",
    "# sub-parts of the U-Net model\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()       \n",
    "        # ConvLSTM part\n",
    "        self.convlstm = ConvLSTM(input_size=(176,176),\n",
    "                                 input_dim=16,\n",
    "                                 hidden_dim=[16],\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 num_layers=1,\n",
    "                                 batch_first=False,\n",
    "                                 bias=True,\n",
    "                                 return_all_layers=True)\n",
    "        # SI part\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(16,1,1,stride=1,padding=0)\n",
    "        self.fc = nn.Linear(16*176*176, 1)\n",
    "        \n",
    "    def forward(self,x,score_bound):\n",
    "        # x.shape() = [batch,C,H,W] H=W=176,is A7\n",
    "        size = x.size()[-1] # W\n",
    "\n",
    "        # convlstm and si\n",
    "        convlstm_input = UP2.unsqueeze(0) #[1,batch,d,h,w],(t, b, c, h, w) -> (b, t, c, h, w) (i.e.[1,1,16,176,176])\n",
    "        output_list = []\n",
    "        scores = []\n",
    "        hidden_state=None\n",
    "        for i in range(380):\n",
    "            [layer_output_list, last_state_list] = self.convlstm(convlstm_input,hidden_state) # layer_output_list=[1,batch,t,d,h,w],t=1,(i.e.[1,1,16,176,176])\n",
    "                                                                                 # last_state_list(i.e. [[h,c]])=[1,2,batch,t,d,h,w] (i.e.[1,2,1,1,16,176,176])\n",
    "            hidden_state = last_state_list\n",
    "\n",
    "            layer_output_list = layer_output_list[0].squeeze()\n",
    "            \n",
    "            # produce score\n",
    "            score_input = layer_output_list.unsqueeze(0)\n",
    "            #score_input = F.max_pool2d(score_input,2) #[88,88]\n",
    "            [b,c,h,w] = score_input.size()\n",
    "            score = F.sigmoid(self.fc(score_input.view(b*c*h*w)))\n",
    "            scores.append(score)\n",
    "            # produce masks\n",
    "            SI = 255*F.sigmoid(\n",
    "                    F.log_softmax(\n",
    "                        self.conv8(layer_output_list.unsqueeze(0)))) #the input is [1,d,h,w],SI is [h,w] (i.e.[16,176,176])\n",
    "\n",
    "            mask = SI.squeeze() #[batch,h,w] ,since batchsize=1,it should be [h,w] (i.e.[176,176])\n",
    "            output_list.append(mask)\n",
    "#             if score < score_bound:\n",
    "#                 print('stop pre for this example')\n",
    "#                 break\n",
    "        return (output_list,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unetdata(Dataset):\n",
    "    \"\"\"Unet dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,root_dir,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the examples.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.example_list = os.listdir(root_dir+'/img')\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.example_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_dir = self.root_dir+'/img'\n",
    "        masks_dir = self.root_dir+'/mask'\n",
    "        \n",
    "        img_name = img_dir+'/'+self.example_list[idx]\n",
    "        image = io.imread(img_name)[:,:,0:3]\n",
    "        \n",
    "        mask_name = masks_dir+'/'+self.example_list[idx]\n",
    "        mask_ = io.imread(mask_name)/255\n",
    "        \n",
    "        sample = {'image': image, 'masks': mask_}# mask_ is [H,W],image [H,W,C]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDataset(Dataset):\n",
    "    \"\"\"Neural dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the examples.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.example_list = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.example_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example_dir = os.path.join(self.root_dir,\n",
    "                                self.example_list[idx])\n",
    "        img_dir = example_dir+'/images'\n",
    "        masks_dir = example_dir+'/masks'\n",
    "        \n",
    "        img_name = img_dir+'/'+os.listdir(img_dir)[0]\n",
    "        image = io.imread(img_name)[:,:,0:3]\n",
    "        \n",
    "        maskwalk = os.walk(masks_dir).__next__()\n",
    "        masks = []\n",
    "        for item in maskwalk[2]:\n",
    "            masks_name = os.path.join(masks_dir,item)\n",
    "            mask = io.imread(masks_name)\n",
    "            masks.append(mask)\n",
    "        masks = np.stack(masks)\n",
    "        \n",
    "        sample = {'image': image, 'masks': masks}# masks is [masknumber,H,W],image [H,W,C]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # resize the masks\n",
    "        new_msk = []\n",
    "        for mask in masks:\n",
    "            new_msk.append(transform.resize(mask, (new_h, new_w)))\n",
    "        new_mask = np.stack(new_msk)\n",
    "        \n",
    "        return {'image': img, 'masks': new_mask}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        if h-new_h==0:\n",
    "            top = 0\n",
    "        else:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        if w-new_w==0:\n",
    "            left = 0\n",
    "        else:\n",
    "            left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        # crop the masks\n",
    "        new_msk = []\n",
    "        for mask in masks:\n",
    "            newm = mask[top: top + new_h,\n",
    "                        left: left + new_w]\n",
    "            new_msk.append(newm)\n",
    "        new_mask = np.stack(new_msk)\n",
    "\n",
    "        return {'image': image, 'masks': new_mask}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, masks = sample['image'], sample['masks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        # numpy masks: N x H x W\n",
    "        # torch masks: N X H X W\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'masks': torch.from_numpy(masks)}\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        # input_tensor is [batch,channel,h,w]\n",
    "        h_cur, c_cur = cur_state     \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        \n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next) # size of h and c is [batch,channel,h,w]\n",
    "        \n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return [Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda(),\n",
    "                Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda()]\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
    "                                          input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo \n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list   = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):  #num_layers should be 2\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])# size of h and c is [batch,c,h,w]\n",
    "                output_inner.append(h)#[t,batch,c,h,w]\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)# layer_output is [batch,t,c,h,w]\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)# [1,batch,t,c,h,w]\n",
    "            last_state_list.append([h, c])#[1,2,batch,t,c,h,w]\n",
    "        torch.cuda.empty_cache()\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list   = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states # [[h,c]]\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "transformed_dataset = Unetdata(root_dir=UNET_PATH,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor()]))\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model\n",
    "unet = UNet(3,1).to(device)\n",
    "# training\n",
    "torch.cuda.empty_cache()\n",
    "# create your optimizer\n",
    "criterion = loss_pre()\n",
    "optimizer = optim.SGD(unet.parameters(), lr=0.1,momentum=0.9,weight_decay=0.0005)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.009\n",
      "[1,    40] loss: 0.014\n",
      "[1] loss: 0.014\n",
      "[2,    20] loss: 0.008\n",
      "[2,    40] loss: 0.004\n",
      "[2] loss: 0.009\n",
      "[3,    20] loss: 0.007\n",
      "[3,    40] loss: 0.009\n",
      "[3] loss: 0.007\n",
      "[4,    20] loss: 0.006\n",
      "[4,    40] loss: 0.005\n",
      "[4] loss: 0.007\n",
      "[5,    20] loss: 0.006\n",
      "[5,    40] loss: 0.005\n",
      "[5] loss: 0.007\n",
      "[6,    20] loss: 0.005\n",
      "[6,    40] loss: 0.007\n",
      "[6] loss: 0.007\n",
      "[7,    20] loss: 0.008\n",
      "[7,    40] loss: 0.005\n",
      "[7] loss: 0.006\n",
      "[8,    20] loss: 0.008\n",
      "[8,    40] loss: 0.008\n",
      "[8] loss: 0.006\n",
      "[9,    20] loss: 0.005\n",
      "[9,    40] loss: 0.005\n",
      "[9] loss: 0.006\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#pre train\n",
    "for epoch in range(9):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    j=0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs\n",
    "        inputs = data['image'].type(torch.float32).to(device)\n",
    "        labels = data['masks'].type(torch.float32).to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        (premask,x64) = unet(inputs)\n",
    "        loss = criterion(premask,labels)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(filter(lambda p:p.requires_grad,net.parameters()), max_norm=5, norm_type=1)\n",
    "        optimizer.step()\n",
    "        #scheduler.step(loss)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        j = j+1\n",
    "        if j % 20 == 19:    # print every 20 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, j+1, loss.item()/16))\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss / 670))\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(unet,'unet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgpu",
   "language": "python",
   "name": "pytorchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
