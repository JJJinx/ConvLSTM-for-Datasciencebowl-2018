{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvLSTM-Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nUlCEwCqG8iH",
        "colab_type": "code",
        "outputId": "42455db4-3ed3-4d56-adf9-4e6f832a23ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131304 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IbxBwBy6HW4y",
        "colab_type": "code",
        "outputId": "c24c41b1-b245-49f6-dec5-1dc1ec978d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.chdir(\"content/drive/\")\n",
        "!ls"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-236-64136c7fc346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/drive/'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "UeYTI9G7_BjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable,Function\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "plt.ion()   # interactive mode\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "colab_type": "code",
        "id": "VEua2vCXIWb5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = './drive/mini_data/DATA/stage1_train/'\n",
        "TEST_PATH = '../drive/mini_data/DATA/stage1_test/'\n",
        "UNET_PATH = './drive/mini_program/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1P8q2hYn_Bjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "qm9qiRs9_Bjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class loss_f(nn.Module):\n",
        "    def __init__(self,lamda=1):\n",
        "        # super parameter\n",
        "        super(loss_f,self).__init__()\n",
        "        self.lamda = lamda\n",
        "    def forward(self,output_list,scores,gt_masks):\n",
        "        \"\"\"\n",
        "        gt_masks: sample['masks'],shape=[1,masks_number,H,W]\n",
        "        output_list: tensor ,size = [premasks_number,H,W]\n",
        "        scores: tensor size = [premasks_numbers,1]\n",
        "        batchsize = 1\n",
        "        \"\"\"\n",
        "        \n",
        "        gt_masks = gt_masks.squeeze(0) # max_element = 1\n",
        "        num_gt = gt_masks.size()[0]\n",
        "        num_pre = output_list.size()[0]\n",
        "        num_min = min(num_gt,num_pre)\n",
        "        loss = 0\n",
        "        Matrix = torch.zeros(num_min,num_gt,device=output_list.device)\n",
        "        for i in range(num_min): # when pre>gt, drop the later prediction\n",
        "            for j in range(num_gt): \n",
        "                #generate a num_min*num_gt matrix \n",
        "                Matrix[i][j] = self._iou(output_list[i],gt_masks[j]) ## IoU is a tensor scalar\n",
        "        \n",
        "        #Matrix = Matrix.cuda()\n",
        "        numpy_m = Matrix.cpu().detach().numpy()\n",
        "        \n",
        "        row_ind,col_ind = linear_sum_assignment(numpy_m)\n",
        "        self.para = (output_list,scores,gt_masks,num_pre,num_gt,num_min,numpy_m,row_ind,col_ind)\n",
        "        for i in range(num_min):\n",
        "            loss = loss + (-Matrix[row_ind[i]][col_ind[i]]+self.lamda*F.binary_cross_entropy(scores[i],torch.tensor([1.0],device=output_list.device)))\n",
        "        for i in range(num_gt,num_pre):#pre> gt时进入 ,pre<gt时不进入\n",
        "            loss = loss + self.lamda*F.binary_cross_entropy(scores[i],torch.tensor([0.0],device=output_list.device)) #F.binary_cross_entropy(b,a)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self,grad_output):\n",
        "        (output_list,scores,gt_masks,num_pre,num_gt,num_min,row_ind,col_ind) = self.para\n",
        "        #grad_hung = torch.zeros(num_pre,device=output_list.device)\n",
        "        grad_s = torch.zeros(num_pre,device=output_list.device)\n",
        "        grad_mask = torch.zeros_like(output_list) #size = [premasks_number,H,W],device=cuda\n",
        "        for t in range(num_pre):\n",
        "            if t<=num_min:\n",
        "                ######hungarian##################\n",
        "                #grad_hung[t] = -numpy_m[row_ind[t]][col_ind[t]] #value of iou\n",
        "                ######hungarian##################\n",
        "                \n",
        "                ######iou########################\n",
        "                X = output_list[row_ind[t]] # pre mask [h,w],device = cuda\n",
        "                Y = gt_masks[col_ind[t]] # gt mask [h,w]\n",
        "                I = torch.sum(torch.mul(X,Y)) # scalar\n",
        "                U = torch.sum(X)+torch.sum(Y)-I # scalar\n",
        "                grad_mask[t] = -1*(U*Y-I*(1-Y))/(U**2) # [h,w]\n",
        "                ######iou########################\n",
        "                grad_s[t] = -self.lamda*(1/(scores[t]))\n",
        "            else:\n",
        "                #grad_mask[t]\n",
        "                grad_s[t] = self.lamda*(1/(1-scores[t]))\n",
        "        # grad_input = [derivate forward(input) wrt parameters] * grad_output\n",
        "        grad_mask = grad_mask*grad_output\n",
        "        grad_s = grad_s*grad_output\n",
        "        print(grad_mask,grad_s)\n",
        "        return grad_mask,grad_s\n",
        "    \n",
        "    def _iou(self,x,y):\n",
        "        iou_inter = torch.sum(torch.mul(x,y))\n",
        "        iou = iou_inter/(torch.sum(x)+torch.sum(y)-iou_inter)\n",
        "        return iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KOr9wjKK_Bj1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Class"
      ]
    },
    {
      "metadata": {
        "id": "YxTmJ_7vMWUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "        # ConvLSTM part\n",
        "        #self.conv = nn.Conv2d(64,16,3,stride=1,padding=1)\n",
        "        self.convlstm = ConvLSTM(input_size=(176,176),\n",
        "                                 input_dim=1,\n",
        "                                 hidden_dim=[1,1],\n",
        "                                 kernel_size=(3, 3),\n",
        "                                 num_layers=2,\n",
        "                                 batch_first=False,\n",
        "                                 bias=True,\n",
        "                                 return_all_layers=True)\n",
        "        # SI part\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(1,1,1,stride=1,padding=0)\n",
        "        self.bias = nn.Parameter(torch.ones([1]),requires_grad=True)\n",
        "    \n",
        "        self.fc = nn.Linear(1*176*176, 1)\n",
        "        #self.up = nn.ConvTranspose2d(1, 1, 2, stride=2,padding=0)\n",
        "        torch.nn.init.uniform_(self.conv8.weight, a=-0.08, b=0.08)\n",
        "        torch.nn.init.uniform_(self.conv8.bias, a=-0.08, b=0.08)\n",
        "\n",
        "    def forward(self, x,seq_len,mode='train'):\n",
        "        \n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x64 = self.up4(x, x1)\n",
        "        x = F.sigmoid(self.outc(x64))\n",
        "        if mode=='pre':\n",
        "            return (x,x64)\n",
        "        # x.shape() = [1,1,176,176]\n",
        "        #x = self.conv(x)#[1,1,176,176]\n",
        "        size = x.size()[-1] # W\n",
        "        # convlstm and si\n",
        "        convlstm_input = x.unsqueeze(0) #[1,batch,d,h,w],(t, b, c, h, w) -> (b, t, c, h, w) (i.e.[1,1,1,176,176])\n",
        "        output_list = []\n",
        "        scores = []\n",
        "        hidden_state=None\n",
        "        for i in range(seq_len):\n",
        "            [layer_output_list, last_state_list] = self.convlstm(convlstm_input,hidden_state) # layer_output_list=[2,batch,t,d,h,w],t=1,(i.e.[2,1,1,1,176,176])\n",
        "                                                                                 # last_state_list(i.e. [[h,c]])=[1,2,batch,t,d,h,w] (i.e.[1,2,1,1,1,176,176])\n",
        "            hidden_state = last_state_list\n",
        "\n",
        "            layer_output_list = layer_output_list[-1].squeeze(1)#[1,1,176,176]\n",
        "            \n",
        "            # produce score\n",
        "            score_input = layer_output_list\n",
        "            #score_input = F.max_pool2d(score_input,2) #[88,88]\n",
        "            [b,c,h,w] = score_input.size()\n",
        "\n",
        "            score = F.sigmoid(self.fc(score_input.view(b*c*h*w)))\n",
        "            scores.append(score)\n",
        "            # produce masks\n",
        "            SI = F.log_softmax(self.conv8(layer_output_list)) #the input is [1,d,h,w],SI is [1,1,h,w] (i.e.[1,1,176,176])\n",
        "            SI = F.sigmoid(SI +self.bias)\n",
        "            #SI = self.up(SI)\n",
        "            mask = SI.squeeze() #[batch,h,w] ,since batchsize=1,it should be [h,w] (i.e.[176,176])\n",
        "            output_list.append(mask)\n",
        "        return (output_list,scores)\n",
        "\n",
        "\n",
        "# sub-parts of the U-Net model\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        \n",
        "        # for padding issues, see \n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJPVbrPq_Bj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper Classes"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XLmhXuhP_Bj-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralDataset(Dataset):\n",
        "    \"\"\"Neural dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the examples.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        #self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.example_list = os.listdir(root_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.example_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example_dir = os.path.join(self.root_dir,\n",
        "                                self.example_list[idx])\n",
        "        img_dir = example_dir+'/images'\n",
        "        masks_dir = example_dir+'/masks'\n",
        "        \n",
        "        img_name = img_dir+'/'+os.listdir(img_dir)[0]\n",
        "        image = io.imread(img_name)[:,:,0:3]\n",
        "        \n",
        "        maskwalk = os.walk(masks_dir).__next__()\n",
        "        masks = []\n",
        "        for item in maskwalk[2]:\n",
        "            masks_name = os.path.join(masks_dir,item)\n",
        "            mask = io.imread(masks_name)\n",
        "            masks.append(mask)\n",
        "        masks = np.stack(masks)\n",
        "        \n",
        "        sample = {'image': image, 'masks': masks,'id_':img_dir}# masks is [masknumber,H,W],image [H,W,C]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "    \n",
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, masks,id_ = sample['image'], sample['masks'],sample['id_']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "\n",
        "        # resize the masks\n",
        "        new_msk = []\n",
        "        for mask in masks:\n",
        "            new_msk.append(transform.resize(mask, (new_h, new_w)))\n",
        "        new_mask = np.stack(new_msk)\n",
        "        \n",
        "        return {'image': img, 'masks': new_mask,'id_':id_}\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, masks,id_ = sample['image'], sample['masks'],sample['id_']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        \n",
        "        new_h, new_w = self.output_size\n",
        "        if h-new_h==0:\n",
        "            top = 0\n",
        "        else:\n",
        "            top = np.random.randint(0, h - new_h)\n",
        "        if w-new_w==0:\n",
        "            left = 0\n",
        "        else:\n",
        "            left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        # crop the masks\n",
        "        new_msk = []\n",
        "        for mask in masks:\n",
        "            newm = mask[top: top + new_h,\n",
        "                        left: left + new_w]\n",
        "            new_msk.append(newm)\n",
        "        new_mask = np.stack(new_msk)\n",
        "\n",
        "        return {'image': image, 'masks': new_mask,'id_':id_}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, masks,id_ = sample['image'], sample['masks'],sample['id_']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        # numpy masks: N x H x W\n",
        "        # torch masks: N X H X W\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'masks': torch.from_numpy(masks),\n",
        "                'id_':id_}\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input_size: (int, int)\n",
        "            Height and width of input tensor as (height, width).\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias        = bias\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "        self.batch = nn.BatchNorm2d(4 * self.hidden_dim)\n",
        "        torch.nn.init.uniform_(self.conv.weight, a=-0.08, b=0.08)\n",
        "        torch.nn.init.uniform_(self.conv.bias, a=-0.08, b=0.08)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        # input_tensor is [batch,channel,h,w]\n",
        "        h_cur, c_cur = cur_state     \n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "        \n",
        "        combined = self.conv(combined)\n",
        "        conbined = self.batch(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined, self.hidden_dim, dim=1) \n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next) # size of h and c is [batch,channel,h,w]\n",
        "        \n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return [Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda(),\n",
        "                Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).cuda()]\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.height, self.width = input_size\n",
        "\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n",
        "                                          input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "        \n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: todo \n",
        "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is None:\n",
        "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):  #num_layers should be 2\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            \n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])# size of h and c is [batch,c,h,w]\n",
        "                output_inner.append(h)#[t,batch,c,h,w]\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)# layer_output is [batch,t,c,h,w]\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)# [1,batch,t,c,h,w]\n",
        "            last_state_list.append([h, c])#[1,2,batch,t,c,h,w]\n",
        "        torch.cuda.empty_cache()\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states # [[h,c]]\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "azHvjJ7p_BkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reading data\n",
        "transformed_dataset = NeuralDataset(root_dir=TRAIN_PATH,\n",
        "                                    transform=transforms.Compose([\n",
        "                                        Rescale(176),\n",
        "                                        RandomCrop(176),\n",
        "                                        ToTensor()]))\n",
        "\n",
        "dataloader = DataLoader(transformed_dataset, batch_size=1,\n",
        "                        shuffle=True, num_workers=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNwJaV3bT3de",
        "colab_type": "code",
        "outputId": "ce47fbe9-757c-4dde-ed02-f700645337d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2591
        }
      },
      "cell_type": "code",
      "source": [
        "#set up model\n",
        "unet = UNet(3,1)\n",
        "if os.path.exists('./drive/mini_program/unet_params.pkl') == False:\n",
        "    pretrained_dict = torch.load('./drive/mini_program/unet.pkl').state_dict()\n",
        "    model_dict = unet.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict) \n",
        "    unet.load_state_dict(model_dict)\n",
        "    print(unet.state_dict)\n",
        "else:\n",
        "    unet.load_state_dict(torch.load('./drive/mini_program/unet_params.pkl'))\n",
        "    print('load the parameters')\n",
        "for i,p in enumerate(unet.parameters()):\n",
        "    if i <72:\n",
        "        p.requires_grad = False\n",
        "unet.bias.requires_grad = True\n",
        "# training\n",
        "unet = unet.to(device)\n",
        "torch.cuda.empty_cache()\n",
        "# create your optimizer\n",
        "criterion = loss_f()\n",
        "optimizer = optim.Adam(unet.parameters(), lr=0.00001)#,momentum=0.9,weight_decay=0.0005)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.state_dict of UNet(\n",
            "  (inc): inconv(\n",
            "    (conv): double_conv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (down1): down(\n",
            "    (mpconv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): double_conv(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace)\n",
            "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (down2): down(\n",
            "    (mpconv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): double_conv(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace)\n",
            "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (down3): down(\n",
            "    (mpconv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): double_conv(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace)\n",
            "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (down4): down(\n",
            "    (mpconv): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (1): double_conv(\n",
            "        (conv): Sequential(\n",
            "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace)\n",
            "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (5): ReLU(inplace)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up1): up(\n",
            "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
            "    (conv): double_conv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up2): up(\n",
            "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
            "    (conv): double_conv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up3): up(\n",
            "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
            "    (conv): double_conv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (up4): up(\n",
            "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
            "    (conv): double_conv(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (outc): outconv(\n",
            "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (convlstm): ConvLSTM(\n",
            "    (cell_list): ModuleList(\n",
            "      (0): ConvLSTMCell(\n",
            "        (conv): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (batch): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): ConvLSTMCell(\n",
            "        (conv): Conv2d(2, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (batch): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv8): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (fc): Linear(in_features=30976, out_features=1, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "g8dIREOL_BkG",
        "colab_type": "code",
        "outputId": "83b22545-7a97-4f05-ddc5-2af95f151a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3635
        }
      },
      "cell_type": "code",
      "source": [
        "file_path = './'\n",
        "for epoch in range(840):  # loop over the dataset multiple times\n",
        "    els = []\n",
        "    running_loss = 0.0\n",
        "    epoch_loss =0.0\n",
        "    k = 0\n",
        "    #converge_loss = np.array([0,0,0,0,0])\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs\n",
        "        inputs = (data['image'].type(torch.float32)).to(device)\n",
        "        labels = data['masks'].type(torch.float32).to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        seq_len = epoch%419+1\n",
        "        (output_list,scores) = unet(inputs,seq_len)\n",
        "        output_list = torch.stack(output_list) # shape = [premasks,H,W]\n",
        "        scores = torch.stack(scores)\n",
        "        \n",
        "        loss = criterion(output_list,scores,labels) #  the minimum of loss is -1\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(filter(lambda p:p.requires_grad,unet.parameters()), max_norm=5, norm_type=1)\n",
        "        optimizer.step()\n",
        "        #scheduler.step(loss)\n",
        "\n",
        "        #judge whether converge\n",
        "#         for j in range(1,5):\n",
        "#             converge_loss[j]=converge_loss[j-1]\n",
        "#         converge_loss[0] = loss.item()\n",
        "#         if converge_loss.max()<-0.65:\n",
        "#             seq_len = seq_len+1\n",
        "#             torch.save(unet.state_dict(), '../mini_program/unet_params.pkl')\n",
        "#             converge_loss = np.array([0,0,0,0,0])\n",
        "        \n",
        "        #print('epoch=',epoch,'num_data=',i,'seq_len=',seq_len,'loss',loss.item(),'lr',optimizer.param_groups[0]['lr'])\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        epoch_loss+= loss.item()\n",
        "        k = k+1\n",
        "        if k % 67 == 66:    # print every 134 mini-batches\n",
        "            print('[%d, %5d] loss: %.8f,seq_len:%d lr:%.10f' %\n",
        "                  (epoch + 1, k + 1, running_loss /67,seq_len,optimizer.param_groups[0]['lr']))\n",
        "            running_loss = 0.0\n",
        "            \n",
        "#     if seq_len>=420:\n",
        "#         break\n",
        "    print('[%d] loss: %.8f,seq_len:%d lr:%.10f' %\n",
        "      (epoch + 1, epoch_loss /670,seq_len,optimizer.param_groups[0]['lr']))\n",
        "    torch.save(unet.state_dict(), './drive/mini_program/unet_params.pkl')\n",
        "#     with open(os.path.join(file_path, \"rls.csv\"),'a') as f:\n",
        "#         f.write(rls)\n",
        "    with open(os.path.join(file_path, \"els.csv\"),'a') as f:\n",
        "        f.write('epoch='+str(epoch+1)+'loss='+str(epoch_loss /670)+'\\n')\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    67] loss: 0.65305278,seq_len:1 lr:0.0000100000\n",
            "[1,   134] loss: 0.59979281,seq_len:1 lr:0.0000100000\n",
            "[1,   201] loss: 0.53420841,seq_len:1 lr:0.0000100000\n",
            "[1,   268] loss: 0.46902846,seq_len:1 lr:0.0000100000\n",
            "[1,   335] loss: 0.40554993,seq_len:1 lr:0.0000100000\n",
            "[1,   402] loss: 0.34459421,seq_len:1 lr:0.0000100000\n",
            "[1,   469] loss: 0.28764544,seq_len:1 lr:0.0000100000\n",
            "[1,   536] loss: 0.23617993,seq_len:1 lr:0.0000100000\n",
            "[1,   603] loss: 0.19077480,seq_len:1 lr:0.0000100000\n",
            "[1,   670] loss: 0.15115173,seq_len:1 lr:0.0000100000\n",
            "[1] loss: 0.38739688,seq_len:1 lr:0.0000100000\n",
            "[2,    67] loss: 0.20076842,seq_len:2 lr:0.0000100000\n",
            "[2,   134] loss: 0.11199258,seq_len:2 lr:0.0000100000\n",
            "[2,   201] loss: 0.08167406,seq_len:2 lr:0.0000100000\n",
            "[2,   268] loss: 0.05702280,seq_len:2 lr:0.0000100000\n",
            "[2,   335] loss: 0.11805749,seq_len:2 lr:0.0000100000\n",
            "[2,   402] loss: 0.02827004,seq_len:2 lr:0.0000100000\n",
            "[2,   469] loss: 0.01945247,seq_len:2 lr:0.0000100000\n",
            "[2,   536] loss: 0.11446592,seq_len:2 lr:0.0000100000\n",
            "[2,   603] loss: 0.00869584,seq_len:2 lr:0.0000100000\n",
            "[2,   670] loss: 0.12766580,seq_len:2 lr:0.0000100000\n",
            "[2] loss: 0.08681481,seq_len:2 lr:0.0000100000\n",
            "[3,    67] loss: 0.00188300,seq_len:3 lr:0.0000100000\n",
            "[3,   134] loss: 0.30440250,seq_len:3 lr:0.0000100000\n",
            "[3,   201] loss: 0.17570531,seq_len:3 lr:0.0000100000\n",
            "[3,   268] loss: 0.49890260,seq_len:3 lr:0.0000100000\n",
            "[3,   335] loss: 0.17676135,seq_len:3 lr:0.0000100000\n",
            "[3,   402] loss: -0.00055710,seq_len:3 lr:0.0000100000\n",
            "[3,   469] loss: -0.00035478,seq_len:3 lr:0.0000100000\n",
            "[3,   536] loss: 0.36862544,seq_len:3 lr:0.0000100000\n",
            "[3,   603] loss: -0.00025485,seq_len:3 lr:0.0000100000\n",
            "[3,   670] loss: 0.36113963,seq_len:3 lr:0.0000100000\n",
            "[3] loss: 0.18862681,seq_len:3 lr:0.0000100000\n",
            "[4,    67] loss: 0.61823169,seq_len:4 lr:0.0000100000\n",
            "[4,   134] loss: -0.00208020,seq_len:4 lr:0.0000100000\n",
            "[4,   201] loss: -0.00219876,seq_len:4 lr:0.0000100000\n",
            "[4,   268] loss: 0.84257647,seq_len:4 lr:0.0000100000\n",
            "[4,   335] loss: 0.63413875,seq_len:4 lr:0.0000100000\n",
            "[4,   402] loss: 0.45727420,seq_len:4 lr:0.0000100000\n",
            "[4,   469] loss: -0.00373327,seq_len:4 lr:0.0000100000\n",
            "[4,   536] loss: 0.63421195,seq_len:4 lr:0.0000100000\n",
            "[4,   603] loss: 1.08433075,seq_len:4 lr:0.0000100000\n",
            "[4,   670] loss: -0.00226180,seq_len:4 lr:0.0000100000\n",
            "[4] loss: 0.42604589,seq_len:4 lr:0.0000100000\n",
            "[5,    67] loss: 0.40634025,seq_len:5 lr:0.0000100000\n",
            "[5,   134] loss: 0.86816067,seq_len:5 lr:0.0000100000\n",
            "[5,   201] loss: 1.05772265,seq_len:5 lr:0.0000100000\n",
            "[5,   268] loss: 0.81921639,seq_len:5 lr:0.0000100000\n",
            "[5,   335] loss: 0.40825871,seq_len:5 lr:0.0000100000\n",
            "[5,   402] loss: 1.24733152,seq_len:5 lr:0.0000100000\n",
            "[5,   469] loss: 1.24599123,seq_len:5 lr:0.0000100000\n",
            "[5,   536] loss: 1.05806665,seq_len:5 lr:0.0000100000\n",
            "[5,   603] loss: -0.00670488,seq_len:5 lr:0.0000100000\n",
            "[5,   670] loss: 2.69948684,seq_len:5 lr:0.0000100000\n",
            "[5] loss: 0.98038186,seq_len:5 lr:0.0000100000\n",
            "[6,    67] loss: 0.40220343,seq_len:6 lr:0.0000100000\n",
            "[6,   134] loss: 1.46841788,seq_len:6 lr:0.0000100000\n",
            "[6,   201] loss: 3.33744298,seq_len:6 lr:0.0000100000\n",
            "[6,   268] loss: 2.05337380,seq_len:6 lr:0.0000100000\n",
            "[6,   335] loss: 1.66795672,seq_len:6 lr:0.0000100000\n",
            "[6,   402] loss: -0.00607191,seq_len:6 lr:0.0000100000\n",
            "[6,   469] loss: 3.50147338,seq_len:6 lr:0.0000100000\n",
            "[6,   536] loss: 1.46809738,seq_len:6 lr:0.0000100000\n",
            "[6,   603] loss: 2.87892545,seq_len:6 lr:0.0000100000\n",
            "[6,   670] loss: 0.40609862,seq_len:6 lr:0.0000100000\n",
            "[6] loss: 1.71777769,seq_len:6 lr:0.0000100000\n",
            "[7,    67] loss: 3.28665563,seq_len:7 lr:0.0000100000\n",
            "[7,   134] loss: 2.87664179,seq_len:7 lr:0.0000100000\n",
            "[7,   201] loss: 2.87832157,seq_len:7 lr:0.0000100000\n",
            "[7,   268] loss: 4.75019583,seq_len:7 lr:0.0000100000\n",
            "[7,   335] loss: 7.83696841,seq_len:7 lr:0.0000100000\n",
            "[7,   402] loss: 0.40101108,seq_len:7 lr:0.0000100000\n",
            "[7,   469] loss: 0.40203791,seq_len:7 lr:0.0000100000\n",
            "[7,   536] loss: 2.46583605,seq_len:7 lr:0.0000100000\n",
            "[7,   603] loss: 1.64121271,seq_len:7 lr:0.0000100000\n",
            "[7,   670] loss: 3.08858721,seq_len:7 lr:0.0000100000\n",
            "[7] loss: 2.96274542,seq_len:7 lr:0.0000100000\n",
            "[8,    67] loss: 8.66619180,seq_len:8 lr:0.0000100000\n",
            "[8,   134] loss: 4.11343485,seq_len:8 lr:0.0000100000\n",
            "[8,   201] loss: 1.22270764,seq_len:8 lr:0.0000100000\n",
            "[8,   268] loss: 1.63727894,seq_len:8 lr:0.0000100000\n",
            "[8,   335] loss: 4.11200539,seq_len:8 lr:0.0000100000\n",
            "[8,   402] loss: 10.75209885,seq_len:8 lr:0.0000100000\n",
            "[8,   469] loss: 2.87254102,seq_len:8 lr:0.0000100000\n",
            "[8,   536] loss: 4.10763588,seq_len:8 lr:0.0000100000\n",
            "[8,   603] loss: 7.82038082,seq_len:8 lr:0.0000100000\n",
            "[8,   670] loss: 2.05057242,seq_len:8 lr:0.0000100000\n",
            "[8] loss: 4.73546221,seq_len:8 lr:0.0000100000\n",
            "[9,    67] loss: 10.12009685,seq_len:9 lr:0.0000100000\n",
            "[9,   134] loss: 6.40607757,seq_len:9 lr:0.0000100000\n",
            "[9,   201] loss: 1.22310667,seq_len:9 lr:0.0000100000\n",
            "[9,   268] loss: 4.11237730,seq_len:9 lr:0.0000100000\n",
            "[9,   335] loss: 10.28598163,seq_len:9 lr:0.0000100000\n",
            "[9,   402] loss: 7.40499656,seq_len:9 lr:0.0000100000\n",
            "[9,   469] loss: 8.23023142,seq_len:9 lr:0.0000100000\n",
            "[9,   536] loss: 4.93253286,seq_len:9 lr:0.0000100000\n",
            "[9,   603] loss: 15.28619234,seq_len:9 lr:0.0000100000\n",
            "[9,   670] loss: 4.93517705,seq_len:9 lr:0.0000100000\n",
            "[9] loss: 7.29361989,seq_len:9 lr:0.0000100000\n",
            "[10,    67] loss: 10.29275132,seq_len:10 lr:0.0000100000\n",
            "[10,   134] loss: 12.35063864,seq_len:10 lr:0.0000100000\n",
            "[10,   201] loss: 11.11737341,seq_len:10 lr:0.0000100000\n",
            "[10,   268] loss: 10.93752882,seq_len:10 lr:0.0000100000\n",
            "[10,   335] loss: 16.47561177,seq_len:10 lr:0.0000100000\n",
            "[10,   402] loss: 2.04217029,seq_len:10 lr:0.0000100000\n",
            "[10,   469] loss: 11.76902123,seq_len:10 lr:0.0000100000\n",
            "[10,   536] loss: 13.00152705,seq_len:10 lr:0.0000100000\n",
            "[10,   603] loss: 4.10384597,seq_len:10 lr:0.0000100000\n",
            "[10,   670] loss: 12.58638375,seq_len:10 lr:0.0000100000\n",
            "[10] loss: 10.46765769,seq_len:10 lr:0.0000100000\n",
            "[11,    67] loss: 23.72342812,seq_len:11 lr:0.0000100000\n",
            "[11,   134] loss: 18.36044914,seq_len:11 lr:0.0000100000\n",
            "[11,   201] loss: 6.57689746,seq_len:11 lr:0.0000100000\n",
            "[11,   268] loss: 10.28813723,seq_len:11 lr:0.0000100000\n",
            "[11,   335] loss: 23.71231117,seq_len:11 lr:0.0000100000\n",
            "[11,   402] loss: 15.23333663,seq_len:11 lr:0.0000100000\n",
            "[11,   469] loss: 12.34663103,seq_len:11 lr:0.0000100000\n",
            "[11,   536] loss: 13.00758401,seq_len:11 lr:0.0000100000\n",
            "[11,   603] loss: 12.35005802,seq_len:11 lr:0.0000100000\n",
            "[11,   670] loss: 9.87174647,seq_len:11 lr:0.0000100000\n",
            "[11] loss: 14.54704139,seq_len:11 lr:0.0000100000\n",
            "[12,    67] loss: 12.75760663,seq_len:12 lr:0.0000100000\n",
            "[12,   134] loss: 30.73086842,seq_len:12 lr:0.0000100000\n",
            "[12,   201] loss: 10.70078785,seq_len:12 lr:0.0000100000\n",
            "[12,   268] loss: 18.94127965,seq_len:12 lr:0.0000100000\n",
            "[12,   335] loss: 21.00277527,seq_len:12 lr:0.0000100000\n",
            "[12,   402] loss: 22.47776892,seq_len:12 lr:0.0000100000\n",
            "[12,   469] loss: 28.01436929,seq_len:12 lr:0.0000100000\n",
            "[12,   536] loss: 16.88273259,seq_len:12 lr:0.0000100000\n",
            "[12,   603] loss: 12.75489333,seq_len:12 lr:0.0000100000\n",
            "[12,   670] loss: 19.77187470,seq_len:12 lr:0.0000100000\n",
            "[12] loss: 19.40347341,seq_len:12 lr:0.0000100000\n",
            "[13,    67] loss: 17.70217633,seq_len:13 lr:0.0000100000\n",
            "[13,   134] loss: 23.29966314,seq_len:13 lr:0.0000100000\n",
            "[13,   201] loss: 27.60121372,seq_len:13 lr:0.0000100000\n",
            "[13,   268] loss: 22.65757291,seq_len:13 lr:0.0000100000\n",
            "[13,   335] loss: 19.35548452,seq_len:13 lr:0.0000100000\n",
            "[13,   402] loss: 23.46947680,seq_len:13 lr:0.0000100000\n",
            "[13,   469] loss: 31.14076041,seq_len:13 lr:0.0000100000\n",
            "[13,   536] loss: 30.89658021,seq_len:13 lr:0.0000100000\n",
            "[13,   603] loss: 31.30704632,seq_len:13 lr:0.0000100000\n",
            "[13,   670] loss: 21.41566865,seq_len:13 lr:0.0000100000\n",
            "[13] loss: 24.88455488,seq_len:13 lr:0.0000100000\n",
            "[14,    67] loss: 26.76685325,seq_len:14 lr:0.0000100000\n",
            "[14,   134] loss: 18.93630703,seq_len:14 lr:0.0000100000\n",
            "[14,   201] loss: 22.64258514,seq_len:14 lr:0.0000100000\n",
            "[14,   268] loss: 39.14958530,seq_len:14 lr:0.0000100000\n",
            "[14,   335] loss: 33.36539768,seq_len:14 lr:0.0000100000\n",
            "[14,   402] loss: 21.40924770,seq_len:14 lr:0.0000100000\n",
            "[14,   469] loss: 37.50386718,seq_len:14 lr:0.0000100000\n",
            "[14,   536] loss: 44.92525630,seq_len:14 lr:0.0000100000\n",
            "[14,   603] loss: 31.30790702,seq_len:14 lr:0.0000100000\n",
            "[14,   670] loss: 30.88151306,seq_len:14 lr:0.0000100000\n",
            "[14] loss: 30.77131441,seq_len:14 lr:0.0000100000\n",
            "[15,    67] loss: 47.79711406,seq_len:15 lr:0.0000100000\n",
            "[15,   134] loss: 22.64442767,seq_len:15 lr:0.0000100000\n",
            "[15,   201] loss: 40.79032400,seq_len:15 lr:0.0000100000\n",
            "[15,   268] loss: 39.54889404,seq_len:15 lr:0.0000100000\n",
            "[15,   335] loss: 37.48946494,seq_len:15 lr:0.0000100000\n",
            "[15,   402] loss: 37.07950594,seq_len:15 lr:0.0000100000\n",
            "[15,   469] loss: 31.29975121,seq_len:15 lr:0.0000100000\n",
            "[15,   536] loss: 41.19926763,seq_len:15 lr:0.0000100000\n",
            "[15,   603] loss: 32.13096963,seq_len:15 lr:0.0000100000\n",
            "[15,   670] loss: 41.20503169,seq_len:15 lr:0.0000100000\n",
            "[15] loss: 37.11843526,seq_len:15 lr:0.0000100000\n",
            "[16,    67] loss: 47.37762507,seq_len:16 lr:0.0000100000\n",
            "[16,   134] loss: 39.96638230,seq_len:16 lr:0.0000100000\n",
            "[16,   201] loss: 16.87427640,seq_len:16 lr:0.0000100000\n",
            "[16,   268] loss: 44.89863342,seq_len:16 lr:0.0000100000\n",
            "[16,   335] loss: 63.05493197,seq_len:16 lr:0.0000100000\n",
            "[16,   402] loss: 35.00726914,seq_len:16 lr:0.0000100000\n",
            "[16,   469] loss: 41.60752592,seq_len:16 lr:0.0000100000\n",
            "[16,   536] loss: 41.61187074,seq_len:16 lr:0.0000100000\n",
            "[16,   603] loss: 42.84849189,seq_len:16 lr:0.0000100000\n",
            "[16,   670] loss: 67.18216108,seq_len:16 lr:0.0000100000\n",
            "[16] loss: 44.04283672,seq_len:16 lr:0.0000100000\n",
            "[17,    67] loss: 18.09073803,seq_len:17 lr:0.0000100000\n",
            "[17,   134] loss: 56.04910365,seq_len:17 lr:0.0000100000\n",
            "[17,   201] loss: 28.81734749,seq_len:17 lr:0.0000100000\n",
            "[17,   268] loss: 53.14506295,seq_len:17 lr:0.0000100000\n",
            "[17,   335] loss: 37.89105190,seq_len:17 lr:0.0000100000\n",
            "[17,   402] loss: 75.83851872,seq_len:17 lr:0.0000100000\n",
            "[17,   469] loss: 56.43606471,seq_len:17 lr:0.0000100000\n",
            "[17,   536] loss: 63.87304294,seq_len:17 lr:0.0000100000\n",
            "[17,   603] loss: 63.88333481,seq_len:17 lr:0.0000100000\n",
            "[17,   670] loss: 57.29416404,seq_len:17 lr:0.0000100000\n",
            "[17] loss: 51.46165509,seq_len:17 lr:0.0000100000\n",
            "[18,    67] loss: 70.48266810,seq_len:18 lr:0.0000100000\n",
            "[18,   134] loss: 36.64652971,seq_len:18 lr:0.0000100000\n",
            "[18,   201] loss: 53.54780631,seq_len:18 lr:0.0000100000\n",
            "[18,   268] loss: 101.39794551,seq_len:18 lr:0.0000100000\n",
            "[18,   335] loss: 70.05589586,seq_len:18 lr:0.0000100000\n",
            "[18,   402] loss: 55.62979187,seq_len:18 lr:0.0000100000\n",
            "[18,   469] loss: 32.53223241,seq_len:18 lr:0.0000100000\n",
            "[18,   536] loss: 48.60596159,seq_len:18 lr:0.0000100000\n",
            "[18,   603] loss: 56.44921187,seq_len:18 lr:0.0000100000\n",
            "[18,   670] loss: 68.00012381,seq_len:18 lr:0.0000100000\n",
            "[18] loss: 59.33475497,seq_len:18 lr:0.0000100000\n",
            "[19,    67] loss: 92.32659317,seq_len:19 lr:0.0000100000\n",
            "[19,   134] loss: 77.06875109,seq_len:19 lr:0.0000100000\n",
            "[19,   201] loss: 63.02685191,seq_len:19 lr:0.0000100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L9WmK3Yvycp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}